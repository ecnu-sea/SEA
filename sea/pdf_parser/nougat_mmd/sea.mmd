# Automated Peer Reviewing in Paper SEA: Standardization, Evaluation, and Analysis

Anonymous ACL submission

###### Abstract

In recent years, the rapid increase in scientific papers has overwhelmed traditional review mechanisms, resulting in varying quality of publications. Although existing methods have explored the capabilities of Large Language Models (LLMs) for automated scientific reviewing, their generated contents are often generic or partial. To address the issues above, we introduce an automated paper reviewing framework SEA. It comprises of three modules: Standardization, Evaluation, and Analysis, which are represented by models SEA-S, SEA-E, and SEA-A, respectively. Initially, SEA-S distills data standardization capabilities of GPT-4 for integrating multiple reviews for a paper. Then, SEA-E utilizes standardized data for fine-tuning, enabling it to generate constructive reviews. Finally, SEA-A introduces a new evaluation metric called mismatch score to assess the consistency between paper contents and reviews. Moreover, we design a self-correction strategy to enhance the consistency. Extensive experimental results on datasets collected from eight venues show that SEA can generate valuable insights for authors to improve their papers.

## 1 Introduction

With the rapid pace of scientific advancement, there has been a significant increase in the volume of research publications (Bornmann and Mutz, 2015; Gao et al., 2024). Nevertheless, it poses considerable challenges for traditional scientific feedback mechanisms (Liang et al., 2023). On one hand, it exacerbates the pressure on the peer review process (Lee et al., 2013; Bjork and Solomon, 2013); on the other hand, the disparate quality of these numerous publications can negatively affect the scientific research milieu (Kelly et al., 2014; Liu and Shah, 2023). Consequently, there is an urgent need for an automated scientific reviewing framework designed to generate constructive reviews with strong evidence supports to help authors improve the caliber of their works (Yuan et al., 2022).

However, the task of delivering timely, thorough, and perceptive feedback on research papers is inherently intricate and cognitively demanding (Horbach and Halffman, 2018). Traditional language models typically struggle to handle such lengthy texts, let alone provide valuable review insights (Cohan et al., 2020; Wang et al., 2020). Fortunately, Large Language Models (LLMs) have demonstrated emergent capabilities (Wei et al., 2022), which have shown state-of-the-art performance in a wide range of natural language tasks (Brown et al., 2020; Touvron et al., 2023; Tan et al., 2024). Further, they have also been strengthened to handle increasingly longer contexts (Jiang et al., 2023), facilitating the possibility for automated reviewing (Liang et al., 2023; Gao et al., 2024).

Currently, some efforts have been made to explore the capabilities of LLMs for automated paper reviewing. For example, Liu and Shah (2023) and Liang et al. (2023) investigate the potential

Figure 1: Multiple reviews of a paper often provide helpful but partial opinions on certain aspects. Integrating these reviews can offer more comprehensive feedback on the paper.

reliability and credibility of paper reviews generated by LLMs with specially designed prompts. Yet most of these LLMs are tailored for broad and general-purpose applications Wei et al. (2023), so simply prompting LLMs in reviewing papers could output generic comments of less value Liang et al. (2023). Further, certain studies have developed peer review datasets and fine-tuned LLMs to learn the paradigm of paper reviewing Wei et al. (2023); Gao et al. (2024). However, in the supervised fine-tuning (SFT) process, these methods simply utilize a review for a paper that can be biased, partial (see Figure 1) and often formalized in various formats and criteria, which could hinder the potential of LLMs for automated paper reviewing Lin et al. (2023); Gao et al. (2024). Also, they lack a self-correction mechanism when the generated reviews are less appealing.

To tackle the issues, in this paper, we propose a novel automated paper reviewing framework, namely, **SEA**, which consists of three modules: **Standardization**, **Evaluation**, and **Analysis**, as shown in Fig. 2. We next summarize the details of each module.

In the **Standardization** module, we develop a model SEA-S, which aims to standardize reviews. Specifically, we first utilize GPT-4 to integrate multiple reviews of a paper into one that is in a unified format and criterion with constructive contents, and form an instruction dataset for SFT. After that, we fine-tune an open-source LLM Mistral-7B to distill the knowledge of GPT-4.

In the **Evaluation** module, we fine-tune another Mistral-7B to derive the SEA-E model, which can comprehensively analyze papers and generate high-quality reviews. Given papers that are in PDF format, we parse them into text and LaTeX codes, and input their corresponding multiple reviews into SEA-S to generate standardized reviews. The parsed papers, standardized reviews and human-crafted prompts constitute another instruction dataset for SFT, leading to SEA-E.

In the **Analysis** module, we further introduce a self-correction strategy that promotes SEA to rethink and regenerate more constructive reviews, when the generated reviews are inconsistent with the parsed papers. To measure the inconsistency, we put forward a metric, namely, _mismatch score_. We also train a regression model SEA-A to estimate scores for the generated reviews. Generally, the larger the scores, the less informative the generated reviews.

Extensive experiments on eight diverse datasets show that the reviews generated by the SEA framework significantly outperform existing methods in terms of quality, comprehensiveness, and consistency. To sum up, we highlight our contributions as follows:

* We propose a novel framework SEA for automated paper reviewing.
* We present an effective model SEA-S for standardizing reviews from various academic venues in different formats and criteria.
* We devise a self-correction strategy to improve the consistency between papers and reviews.
* We conduct extensive experiments to show the superiority of SEA over other competitors.

_Finally, we emphasize that the goal of this paper is to provide informative reviews for authors to polish their papers instead of directly recommending acceptance/rejection on papers._

## 2 Related Works

### Long-context Large Language Models

LLMs have recently achieved substantial progress in accommodating lengthy contexts. For example, LongLLAMA Tworkowski et al. (2024) and LongLoRA Chen et al. (2023) support long contexts processing by modifying the attention mechanism. There are also some positional encoding methods proposed, including ALiBi Press et al. (2021), xPOS Sun et al. (2022) and RoPE variants Chen et al. (2023); Xiong et al. (2023).

Assessing the capability of LLMs in handling long contexts has also attracted significant attention. The needle-in-a-Haystack (NIAH) test Kamradt (2023) has been widely adopted to evaluate long-context LLMs. Further, RULER Hsieh et al. (2024) extends the vanilla NIAH test to provide a more thorough assessment. Based on the RULER evaluation results, we select Mistral-7B Jiang et al. (2023) as the base model in our paper. Mistral-7B is a compact LLM that has been shown to handle at least 16K tokens, sufficient to meet the input requirements of most academic papers.

### Automated Scientific Reviewing

Automating scientific reviewing began its investigation in the era of small language models. The early work Zhang et al. (2022) utilizes RoBERTa Liu et al. (2019) to assess the textual fluency of papers and fairness disparity in peer review. In peer grading Morris et al. (2023) fine-tune distilBERT Sanh et al., 2019) using course grading data from massive open online courses to examine the reliability of peer grading scores. However, due to the restricted capability of language models in handling lengthy contexts, automating scientific reviewing of a full paper has not been studied before the advent of LLMs.

Recently, since LLMs exhibit advancements in various NLP tasks, some studies are exploring the capabilities of LLMs in automated paper reviewing. For example, Liu and Shah (2023) and Liang et al. (2023) customize prompts to guide GPT-4 in generating scientific feedbacks. Wei et al. (2023) conduct continuous training of LLaMA2-70B (Touvron et al., 2023) on academic data, resulting in an academically enhanced model AcademicGPT. Further, Gao et al. (2024) collect a large-scale peer review dataset, and propose a two-stage review generation framework REVIEWER2 with question-guided prompts.

## 3 SEA

This section details three major modules (i.e., Standardization, Evaluation and Analysis) of SEA, and the overall framework is illustrated in Figure 2.

### SEA-S: Standardization

To explore the potential of LLMs in automated scientific reviewing, a high-quality labeled dataset is generally needed for supervised fine-tuning (SFT). This process feeds LLMs with more peer reviews, thereby enhancing the quality of its generated ones. However, in the peer review datasets, each paper is often associated with multiple peer reviews, with each review offering a limited perspective based on the reviewer's field and expertise. On the other hand, the review formats and criteria could vary across different academic venues, and directly performing SFT on existing peer review datasets can lead to inconsistencies. Therefore, we first have to standardize reviews in a unified format and criterion with comprehensive contents before SFT. For each paper, we integrate all the reviews into one, which can eliminate redundancy and error in multiple reviews. The integrated review is expected to focus on the major advantages and disadvantages of the paper, thereby enhancing its quality.

To perform data standardization, we attempt several representative open-source and closed-source models, such as Mistral-7B, GPT-3.5 and GPT-4. We empirically observe that Mistral-7B and GPT-3.5 tend to simply concatenate the original contents. In contrast, GPT-4 leads them by integrating reviews in an unified format and providing detailed evidence for each argument (The comparative examples are given in Figure 6 of Appendix A.1). However, the API for GPT-4 is costly and inflexible. Inspired by Alpaca (Taori et al., 2023), we distill GPT-4's excellent data standardization capabilities into open-source models.

Specifically, we first randomly select 20% of the papers from the training set along with their reviews \(\{[r_{i1}^{\text{origin}},r_{i2}^{\text{origin}},\ldots,r_{im}^{\text{origin}} ]\}_{i=1}^{n}\), where \(n\) is the number of selected papers and \(m\) is the number of reviews corresponding to paper \(p_{i}\). Next, for each paper \(p_{i}\), we input all its reviews along with the customized instruction \(inst_{s}\) into GPT-4, which in turn yields the standardized re

Figure 2: The overall framework of SEA consists of three modules: Standardization, Evaluation and Analysis.

 view \(r_{i}^{\text{GPI-4}}\). In this way, we can construct the _instruction dataset_ for the data standardization model SEA-S that takes Mistral-7B as the base model. Formally, the triplet in the dataset is \(\textless{inst}_{s},[r_{i1}^{\text{origin}},r_{i2}^{\text{origin}},\dots,r_{im}^ {\text{origin}}],r_{i}^{\text{GPI-4}}\), which is further served for SFT. After fine-tuning SEA-S, we feed all the reviews in the training set into SEA-S for data standardization, which outputs the integrated reviews \(\{r_{i}^{\text{SEA-S}}\}_{i=1}^{N}\). Here, \(N\) denotes the number of papers in the training set. In summary, SEA-S provides a novel paradigm for integrating peer review data in an unified format across various conferences.

### SEA-E: Evaluation

In the Evaluation module, we aim to construct a talented LLM that can deeply understand papers and generate constructive reviews. Notably, since raw crawled papers are in PDF format, we first apply Nougat (Blecher et al., 2023) as the parser, which is a model based on Visual Transformer and is specially designed for parsing academic documents. In particular, Nougat can parse formulas into LaTeX codes instead of corrupted text encoding, enabling LLMs to gain a deeper understanding of papers' contents. Further, due to the long-text characteristic of papers, we choose the open-source model Mistral-7B as the backbone model, which has demonstrated its ability in effectively handling up to 16K tokens for the long-context benchmark RULER (Hsieh et al., 2024).

Based on the outputs of the SEA-S model, we next construct the _instruction dataset_ for the evaluation model SEA-E. Each triplet in the dataset is denoted as \(\textless{inst}_{e},\hat{p}_{i},r_{i}^{\text{SEA-S}}\), where \(inst_{e}\) is the specially designed instruction for evaluation, \(\hat{p}_{i}\) is the parsed paper, and \(r_{i}^{\text{SEA-S}}\) is the standardized review. Note that \(r_{i}^{\text{SEA-S}}\) contains solid evidence for each argument in the review. This endows SEA-E with the capability to generate comprehensive and constructive reviews after SFT.

### SEA-A: Analysis

Now, we step into the Analysis module, where a _mismatch score_ is proposed to measure the consistency between papers and their generated reviews. Given a paper \(p\) with \(m\) raw reviews, let us denote its ground-truth _paper ratings_ as \(S_{p}=\{s_{pr_{1}},s_{pr_{2}},\dots,s_{pr_{m}}\}\) and _confidence scores_ as \(C_{p}=\{c_{pr_{1}},c_{pr_{2}},\dots,c_{pr_{m}}\}\), where each \(s_{pr_{i}}\) and \(c_{pr_{i}}\) indicate the rating and confidence score given by the \(i\)-th reviewer. We next use the confidence scores as weights and calculate the weighted average rating of paper \(p\), which is further subtracted from the reviewer's rating to serve as the ground truth mismatch score. Formally, we have:

\[y_{true}^{pr_{i}}=s_{pr_{i}}-\frac{\sum_{j=1}^{m}c_{pr_{j}}*s_{pr_{j}}}{\sum_{j =1}^{m}c_{pr_{j}}}. \tag{1}\]

From the equation, we see that, when a reviewer's rating is greater than the weighted average, the review may tend to emphasize the paper's strengths; otherwise, the review may be preferably critical of the paper. Generally, the greater the difference, the lower the review quality. When \(y_{true}^{pr_{i}}=0\), we consider the review to be relatively neutral and consistent with the paper content. For example, when the review ratings of a paper are {2, 6, 6, 6} and all are given with full confidence, the quality of the review rated 2 is considered to be lower because it deviates significantly from the weighted average rating of 5.

To estimate the mismatch score, we train a lightweight regression model SEA-A. Specifically, each parsed paper \(\hat{p}\) and its corresponding review \(r\) generated from SEA-E form a pair \(\textless{\hat{p}},r\textgreater\), which serves as the input. We first utilize the pre-trained sentence representation model SFR-Embedding-Mistral (Rui Meng, 2024) that is designed for long contexts to transform the texts of papers and reviews into representations \(h_{\hat{p}}\) and \(h_{r}\), respectively. Then, we compute the _query_ and _key_ vectors for both the paper and the review separately:

\[\begin{split} q_{\hat{p}}&=W^{q}h_{\hat{p}},\quad q_ {r}=W^{q}h_{r},\\ k_{\hat{p}}&=W^{k}h_{\hat{p}},\quad k_{r}=W^{k}h_{r}. \end{split} \tag{2}\]

Here, \(W^{q}\) and \(W^{k}\) are learnable weight matrices. Based on the query and key vectors, we calculate the estimated mismatch score \(y_{pred}^{pr}\) by:

\[y_{pred}^{pr}=w(q_{\hat{p}}{k_{r}}^{T}+q_{r}{k_{\hat{p}}}^{T})+b. \tag{3}\]

Finally, we use the mismatch score \(y_{true}^{pr}\) as the ground truth and the Mean Squared Error (MSE) loss as the objective to train the regression model SEA-A. The smaller the absolute value of the mismatch score, the higher the consistency between the review and the paper.

After SEA-A is trained, we further introduce a _self-correction strategy_ to analyze each review generated by SEA-E. When the estimated mismatch score \(y_{pred}^{pr}\) is larger than a pre-set threshold \(\theta\), we regenerate the review by adding the current mismatch score as additional prompt to ensure the consistency between the paper and the review.

 
## 4 Experiments
 Baselines.We compare the following baseline methods, which are divided into two categories: (1) Direct inference with LLMs: We directly use **M**istral-**7B** (**M-7B**) for inference, guided by \(inst_{e}\) to generate reviews in the specified format. (2) SFT methods: From all reviews for each paper in the training set, we randomly select one review as the output for SFT, referred to as **M**istral-**7B**-**R**andom (**M-7B-R**). In addition, _gpt-3.5-turbo_ is used to standardize all reviews for each paper, which is then used as the output in the SFT stage. We call this method **M**istral-**7B**-GPT-**3.5** (**M-7B-3.5**).

We unify the instruction \(inst_{e}\) and input \(\hat{p}\) across all the baseline methods and our framework. Here, \(inst_{e}\) is the instruction for SEA-E, and \(\hat{p}\) represents the parsed paper. Detailed information about \(inst_{e}\) can be found in Table 8 in Appendix A.1.

### Main Results

We use BLEU (Papineni et al., 2002), ROUGE (Recall), ROUGE (F1-score) (Lin, 2004), and BERTScore (Zhang et al., 2019) as metrics to evaluate the quality of generated reviews across eight datasets. Specifically, BLEU and ROUGE measure the similarity between papers and reviews based on n-grams, while BERTScore focuses on semantic similarity in the embedding space. For the ROUGE metric, recall measures how comprehensively the generated reviews capture the key information from raw papers, while the F1 score assesses the balance between precision and recall in the generated contents. To measure the completeness and comprehensiveness of the generated reviews, we simply concatenate all the reviews of each paper to serve as a benchmark for evaluation. Moreover, we have also counted the average number of tokens in the generated reviews.

The results in Table 2 show that SEA outperforms other baseline models across all the testing scenarios, with particularly notable gains on the ROUGE (Recall) metric. This confirms that our proposed framework SEA is capable of generating comprehensive and constructive reviews. Further, SEA not only performs excellently on in-domain tasks but also shows strong performance on cross-domain datasets, demonstrating its robust generalizability. It is also worth noting that SEA-EA surpasses SEA-E in all cases, underscoring the effectiveness of the self-correction strategy in generating well-grounded reviews consistent with raw papers. However, for M-7B-R, we notice that randomly selecting a review as the output of SFT often leads to shorter texts. To some extent, the quality of a review is positively correlated with its length, which explains its poor performance. Although directly inferring with M-7B can generate longer text, it fails to align with human reviews, resulting in lower evaluation scores. For M-7B-3.5, its performance is poorer than SEA-E, which further indicates the effectiveness of SEA-S. Consequently, using high-quality standardized data generated by SEA-S can effectively improve the performance of SFT. In Appendix A.2 we give concrete examples of reviews generated by different models.

### Comparison of Standardized Results

We show the standardized results on papers in the training set of NeurIPS-2023 and ICLR-2024 that have different rating criteria. In addition, reviews are organized in various formats.

Content analysis.We first compare SEA-S with Mistral-7B, GPT-3.5, and GPT-4 to evaluate their review standardization performance. All the models are fed with the same inputs, including the instruction \(inst_{s}\) and multiple reviews. Since there is no ground-truth text for this standardized task, we utilize reviews generated by SEA-S as _references_, while reviews generated by other models serve as _candidates_. Next, we calculate _recall_ and _precision_ values of ROUGE for candidates compared to references. Based on the content intersection of reference and candidate, recall and precision refer to the percentage of intersection in reference and candidate, respectively. From the two metrics, we can deduce the percentages of overlapping and exclusive semantic information in both reviews, whose results are shown in Figure 3. We compare the model performance w.r.t. different ROUGE metrics, including ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L (RL). The light blue area in the figure indicates the overlapping contents, while the dark blue and light grey areas represent the exclusive contents by SEA-S (reference) and other models (candidate), respectively.

From the figure, we see that, SEA-S can generate a significantly larger percentage of exclusive contents than both Mistral-7B and GPT-3.5. This further verifies that SEA-S can better standardize reviews with richer information. We also surprisingly observe that SEA-S can output slightly more exclusive contents in standardized reviews than GPT-4. The reason could be that the instruction dataset for SFT in SEA-S is derived from GPT-4.

 * [475] Considering the high cost of GPT-4, this demonstrates the effectiveness of small models for review standardization. On the other hand, recap that the difference between M-7B-3.5 and SEA-E only lies in the data standardization step. The advantage of SEA-E over M-7B-3.5 in Table 2 shows that SEA-S has better data standardization capability.

[MISSING_PAGE_POST]

* [4848 from \([1,4]\), and "Rating", which is an integer from \([1,10]\). The rating criterion is given in the instruction of SEA-E in Table 9. In practice, each paper has multiple reviews and each review has the above four scores. Therefore, given a paper, for each score, we use the "Confidence" score in each review as the weight and calculate the weighted average as the reference score.

To assess the discrepancy between the generated scores and the reference scores, we use the Mean Squared Error (MSE) metric. The lower the MSE value, the more accurate the generated results. In Table 3, the percentages in parentheses indicate the proportions of generated reviews with valid scores, while "N/A" denotes those with unsuccessful generations (e.g. text is generated instead of scores). It can be seen that our proposed method ensures the validity of the output format, whereas other models tend to generate content that does not comply with the instruction to varying degrees, especially M-7B that has not undergone SFT. The MSE metric shows that our proposed methods outperform the baseline models in practically all cases. Although SEA-E scores larger than M-7B-3.5 by 0.02 in the "Pre- sentation" on ICLR-2024, SEA-E achieves 100% valid scores in generation, whereas M-7B-3.5 only reaches 86%. Additionally, SEA-EA demonstrates improvements over SEA-E in most cases, further validating that a self-correcting strategy allows for high consistency between generated results and human feedback on quantitative evaluation results.

### Qualitative Decision Analysis

In this part, we analyze "Decision" and "Reason" of the generated review, i.e., the final decision (accept or reject) of the paper and the corresponding reasons. Typically, the Area Chair (AC) gives the final decision and meta-reviews. We calculate the accuracy, precision, recall, and F1-score of the generated results compared to the final decisions, and use BERTScore to measure the semantic similarity between the reasons and meta-reviews. The model M-7B-R randomly selects a review for SFT that does not include the decision or the meta reviews, hence we do not take it as baseline.

From Table 4, it can be seen that SEA-EA leads to the largest accuracy and BERTScore values, where the latter shows the model's effectiveness in generating reasons semantically aligned with meta-reviews. Due to the acceptance rate of 95% in the NeurIPS-2023 test set (see Table 1), the overall results are large. For ICLR-2024, the accuracy of SEA-EA surpasses that of SEA-E over 4%, further indicating the effectiveness of the self-correction strategy. Additionally, we note that M-7B exhibits high recall about 97%, but poor precision, suggesting a tendency to cater to human preferences by accepting most papers. In contrast, our method performs better in both Precision and F1-score, which indicates that ours can identify papers of different quality more effectively. Overall, SEA aligns more closely with actual AC decisions and refrains from favoring decisions that lean towards acceptance.

## 5 Conclusion

In this paper, we presented SEA, a novel framework for automated paper reviewing based on three modules: Standardization, Evaluation, and Analysis. Specifically, we proposed a new paradigm for constructing a standardized review dataset. Based on this dataset, we can fine-tune the long-context LLM to generate high-quality reviews. Moreover, we proposed a new evaluation metric to measure the consistency between papers and generated reviews. Comprehensive experimental results on eight datasets demonstrate that the SEA framework can generate feedback that aligns with human reviews. We anticipate that the SEA framework will help researchers improve the quality of their work and shed light on the field of automated scientific reviewing.

\begin{table}
\begin{tabular}{l l c c c c c} \hline \hline  & Method & Accuracy & Precision & Recall & F1-score & BERTScore \\ \hline \multirow{4}{*}{} & M-7B* & 93.18 & 94.01 & 99.05 & 96.47 & 84.27 \\  & M-7B-3.5* & 81.01 & 95.34 & 83.91 & 98.26 & 84.04 \\  & SEA-E & 99.41 & 99.37 & **100.0** & 99.69 & 84.21 \\  & SEA-EA & **99.70** & **99.69** & **100.0** & **99.84** & **85.22** \\ \hline \multirow{4}{*}{} & M-7B* & 36.81 & 37.14 & **97.65** & 53.82 & 84.19 \\  & M-7B.35* & 50.27 & 39.63 & 61.03 & 48.06 & 84.61 \\ \cline{1-1}  & M-7B-3.5 & 54.16 & 43.31 & 69.95 & 53.50 & 85.07 \\ \cline{1-1}  & SEA-EA & **58.23** & **46.48** & 71.36 & **56.30** & **86.08** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Qualitative Decision Analysis. The symbol (*) indicates that there are incompleteness or errors in the generated content; only valid generations are counted.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & Method & Soundness & Presentation & Contribution & Rating \\ \hline \multirow{4}{*}{} & M-7B & N/A & N/A & N/A & 8.51 (10\%) \\  & M-7B-R & 0.20 (99\%) & 0.26 (99\%) & 0.32 (99\%) & 1.44 (99\%) \\  & M-7B-3.5 & 0.15 (99\%) & 0.16 (99\%) & 0.27 (99\%) & 1.14 (99\%) \\  & SEA-E & 0.12 (100\%) & **0.14 (100\%)** & 0.18 (100\%) & 0.80 (100\%) \\  & SEA-EA & **0.11 (100\%)** & 0.15 (100\%) & **0.17 (100\%)** & **0.73 (100\%)** \\ \hline \multirow{4}{*}{} & M-7B & N/A & N/A & N/A & 12.96 (13\%) \\  & M-7B-R & 0.32 (99\%) & 0.39 (99\%) & 0.42 (99\%) & 2.12 (99\%) \\ \cline{1-1}  & M-7B-3.5 & 0.32 (86\%) & 0.28 (86\%) & 0.45 (86\%) & 2.50 (86\%) \\ \cline{1-1}  & SEA-E & 0.28 (100\%) & 0.30 (100\%) & 0.38 (100\%) & 2.11 (100\%) \\ \cline{1-1}  & SEA-EA & **27 (100\%)** & **0.24 (100\%)** & **0.34 (100\%)** & **1.72 (100\%)** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Quantitative Score Analysis.

 

#### Limitations

Despite these notable achievements, it is crucial to acknowledge the limitations of SEA, particularly its limited expansion into various academic disciplines and insufficient alignment with human standards. Here we elaborate on some of these constraints, along with intriguing future explorations.

#### Domain Expansion.

Although the SEA framework has been successful in automating paper review generation within the machine learning field, it has not yet been expanded to other academic disciplines, such as physics and mathematics. As a universal automated paper review framework, SEA is able to generalize across any field. Thus, it would be exhilarating to investigate whether SEA can yield high-quality review feedback when applied to other academic disciplines.

#### Enhanced Consistency-Guided Training.

Although optimizing the output of SEA-E by calculating mismatch scores between review and the original paper can generate review that are more consistent in content, we did not enhance SEA-E using natural language guidance based on scores during the training phase. To improve SEA-E in following instructions during the self-correction phase, we plan to collect relevant natural language guided self-correction dataset. By training on this dataset, we will further enhance SEA-E in content preference, enabling it to generate review feedback that aligns more accurately with the original paper.

#### Rebuttal Exploration.

In the academic peer review process, the rebuttal stage is a critical component. During this stage, authors have the opportunity to correct potential misunderstandings by reviewers, clarify specific parts of their paper, or provide additional data and information to enhance the support for their research findings. Therefore, in our future research, we will explore methods to assist authors in making effective rebuttals.

#### Ethical Considerations

This paper proposes an automated paper reviewing framework that utilizes advanced long-context LLMs and supervised fine-tuning to align with human reviews and generate comprehensive reviews. This assists authors in improving the quality of their papers. As we explore the extensive potential of automated paper reviewing, it is essential to consider potential consequences associated with this technology. A significant concern is the misuse of the model. In the formal review processes of academic conferences, authors may receive reviews generated by the model without their knowledge. This situation could not only impact the fairness and transparency of the review process but also raise issues of trust and authenticity. To mitigate these risks, we will incorporate specific clauses in our usage license that strictly prohibit any misuse of the system, thereby ensuring it serves as a beneficial tool in academia.

## References

* [1]Bo-Christer Bjork and David Solomon. 2013. The publishing delay in scholarly peer-reviewed journals. _Journal of informetrics_, 7(4):914-923.
* [2] Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. 2023. Nougat: Neural optical understanding for academic documents. In _The Twelfth International Conference on Learning Representations_.
* [3] Lutz Bornmann and Rudiger Mutz. 2015. Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references. _Journal of the association for information science and technology_, 66(11):2215-2222.
* [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901.
* [5] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023a. Extending context window of large language models via positional interpolation. _arXiv preprint arXiv:2306.15595_.
* [6] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023b. Longlora: Efficient fine-tuning of long-context large language models. _arXiv preprint arXiv:2309.12307_.
* [70] Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel Weld. 2020. SPECTER: Document-level representation learning using citation-informed transformers. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 2270-2282, Online. Association for Computational Linguistics.
* [71] Nils Dycke, Ilia Kuznetsov, and Iryna Gurevych. 2022. Nlpeer: A unified resource for the computational study of peer review. _arXiv preprint arXiv:2211.06651_.
* [72] Zhaolin Gao, Kiante Brantley, and Thorsten Joachims. 2024. Reviewer2: Optimizing review generation through prompt generation. _arXiv preprint arXiv:2402.10886_.
* [73] SPJM ( Serge) Horbach and W ( Willem) Halffman. 2018. The changing forms and expectations of peer review. _Research integrity and peer review_, 3:1-15.
* [74] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. 2024. Ruler: What's the real context size of your long-context language models? _arXiv preprint arXiv:2404.06654_.
* [75] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillamme Lample, Lucile Saulnier, et al. 2023. Mistral 7b. _arXiv preprint arXiv:2310.06825_.
* [76] G Kamradt. 2023. Needle in a haystack-pressure testing l1ms.
* [77] Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine Van Zuylen, Sebastian Kohlmeier, Eduard Hovy, and Roy Schwartz. 2018. A dataset of peer reviews (peererad): Collection, insights and nlp applications. _arXiv preprint arXiv:1804.09635_.
* [78] Jacalyn Kelly, Tara Sadeghieh, and Khosrow Adeli. 2014. Peer review in scientific publications: benefits, critiques, & a survival guide. _Ejifcc_, 25(3):227.
* [79] Carole J Lee, Cassidy R Sugimoto, Guo Zhang, and Blaise Cronin. 2013. Bias in peer review. _Journal of the American Society for information Science and Technology_, 64(1):2-17.
* [80] Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Smith, Yian Yin, et al. 2023. Can large language models provide useful feedback on research papers? a large-scale empirical analysis. _arXiv preprint arXiv:2310.01783_.
* [81] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out_, pages 74-81.
* [82] Jialiang Lin, Jiaxin Song, Zhangping Zhou, Yidong Chen, and Xiaodong Shi. 2023. Moprd: A multidisciplinary open peer review dataset. _Neural Computing and Applications_, 35(34):24191-24206.
* [83] Ryan Liu and Nihar B Shah. 2023. Reviewergrpt? an exploratory study on using large language models for paper reviewing. _arXiv preprint arXiv:2306.00622_.
* [84] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_.
* [85] Wesley Morris, Scott Crossley, Langdon Holmes, and Anne Trumbore. 2023. Using transformer language models to validate peer-assigned essay scores in massive open online courses (moocs). In _LAK23: 13th international learning analytics and knowledge conference_, pages 315-323.
* [86] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, pages 311-318.
* [87] Ofir Press, Noah Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. In _International Conference on Learning Representations_.
* [88] Shafiq Rayhan Joty Caiming Xiong Yingbo Zhou Semih Yavuz Rui Meng, Ye Liu. 2024. Sfr-embedding-mistral:enhance text retrieval with transfer learning. Salesforce AI Research Blog.
* [89]* Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. _arXiv preprint arXiv:1910.01108_.
* Sun et al. (2022) Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022. A length-extrapolatable transformer. _arXiv preprint arXiv:2212.10554_.
* Tan et al. (2024) Keren Tan, Kangyang Luo, Yunshi Lan, Zheng Yuan, and Jinlong Shu. 2024. An llm-enhanced adversarial editing system for lexical simplification. _arXiv preprint arXiv:2402.14704_.
* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpaca: A strong, replicable instruction-following model. _Stanford Center for Research on Foundation Models. https://crfin. stanford. edu/2023/03/13/alpaca. html_, 3(6):7.
* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_.
* Tworkowski et al. (2024) Szymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Milos. 2024. Focused transformer: Contrastive training for context scaling. _Advances in Neural Information Processing Systems_, 36.
* Wang et al. (2020) Qingyun Wang, Qi Zeng, Lifu Huang, Kevin Knight, Heng Ji, and Nazneen Fatema Rajani. 2020. ReviewRobot: Explainable paper review generation based on knowledge synthesis. In _Proceedings of the 13th International Conference on Natural Language Generation_, pages 384-397, Dublin, Ireland. Association for Computational Linguistics.
* Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. _Transactions on Machine Learning Research_.
* Wei et al. (2023) Shufa Wei, Xiaolong Xu, Xianbiao Qi, Xi Yin, Jun Xia, Jingyi Ren, Peijun Tang, Yuxiang Zhong, Yi-hao Chen, Xiaoqin Ren, et al. 2023. Academicgpt: empowering academic research. _arXiv preprint arXiv:2311.12315_.
* Xiong et al. (2023) Wenhan Xiong, Jingyu Liu, Igor Molybo, Hejia Zhang, Prajiwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. 2023. Effective long-context scaling of foundation models. _arXiv preprint arXiv:2309.16039_.
* Yuan et al. (2022) Weizhe Yuan, Pengfei Liu, and Graham Neubig. 2022. Can we automate scientific reviewing? _Journal of Artificial Intelligence Research_, 75:171-212.
* Zhang et al. (2022) Jiayao Zhang, Hongming Zhang, Zhun Deng, and Dan Roth. 2022. Investigating fairness disparities in peer review: A language model enhanced approach. _arXiv preprint arXiv:2211.06398_.
* Zhang et al. (2019) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. _arXiv preprint arXiv:1904.09675_.

 More Detailed Description of the Framework SEA

### SEA-S

We further analyse the performance of SEA-S, the open-source model Mistral-7B, and the closed-source models GPT-3.5 and GPT-4 in standardised review experiments.

Instruction.In Table 8, we demonstrate our instructions for generating standardized review based on multiple reviews for each paper. We specify in the instruction that the model should integrate multiple reviews into three parts: textual descriptions, quantitative scores, and review results. The textual descriptions include "Summary", "Strengths", "Weaknesses", and "Questions", while the quantitative scores cover "Soundness", "Presentation", "Contribution", and "Rating". These elements are formatted in alignment with the original review template. Additionally, we incorporate the Area Chair's (AC) decision into the generated content, and instruct the model to generate corresponding acceptance or rejection reasons.

Standardization Examples.Figure 6 shows standardization examples from Mistral-7B, GPT-3.5, and SEA-S, which incorporate multiple reviews for the same paper. We can observe from the figure that the output of SEA-S is both rich and concise without redundant information. In contrast, the output from Mistral-7B not only lacks complete format but also has sparse content, with the missing parts highlighted in orange in the figure. As for the review generated by GPT-3.5, a significant portion consists merely of straightforwardly extracting original review content, failing to eliminate redundant information as instructed. such as the overuse of the phrase "Lack of", which is indicated in red to show the excessive repetition.

### SEA-E

In Table 9, we present the instruction designed to generate reviews that conform to the specified format based on the content of the paper. In Figures 7 and 8, we display the reviews generated by different models for a particular paper, including Mistral-7B (M-7B), Mistral-7B-Random (M-7B-R), Mistral-7B-GPT-3.5 (M-7B-3.5), SEA-E, and SEA-EA. We can observe the following points: (1) Mistral-7B raises broad and general issues, tending to please humans. In the "Strengths" part, it splits the complexity issue into two points, which is not concise, and the content of the "Weaknesses" part does not match the paper decision. (2) Mistral-7B-Random visibly generates shorter texts with reduced detail. (3) Mistral-7B-GPT-3.5 generates duplicates due to insufficient standardization of the instruction dataset at the SFT stage, resulting in lower-quality reviews. (4) SEA-E and SEA-EA generate clearer viewpoints and ensure extensive coverage of content. (5) SEA-EA focuses more on the details within the paper. These comparisons demonstrate the superiority of SEA-E and SEA-EA in generating reviews.

 * [22] A. K. K.

 

**Summary.**

This paper proposes 200, a method for video stylization using 3D CNN encoder backbone and 200 for separating motion and appearance features. The authors demonstrate the effectiveness of the proposed method on the 200 dataset.

**"Extensible."**

First to use 3D CNN for video stylization - Temporally consistent results - Quantitative evaluations of optical flow

**"Weaknesses."**

Motivations for using 3D CNN not well explained - Usage of 3D CNN brings extra computational cost - Style clip could be processed using a 2D CNN - [https://cp.coinstitutionaryment](https://cp.coinstitutionaryment) code be improved - Model could be less elegant with multiple training phases - Comparison with SOTAs not comprehensive

**(Jack of Questions)**

**"Paper Decisions."**

**"Decision: Accept - Reasons: The paper presents a novel approach to video stylization using a 3D CNN encoder and 200K for motion and appearance separation. While there are some weaknesses, such as the lack of clear modulations for using 3D CNN and the need for comprehensive comparison with SOTAs, the results demonstrate state-of-the-art performance. The authors are encouraged to address the weaknesses in future work.

**"Summary."**

This paper introduces 200, a method for video stylization using 3D CNNs and 200K to disentangle motion and appearance. The proposed method achieves state-of-the-art results on 200K datasets. While reviewers acknowledge the novelty and effectiveness of the approach, there are concerns about the motivation for using 3D CNN, model complexity, and the need for more comprehensive comparison.

**"Extensible."**

This paper is the first to use 3D CNN for video stylization - The temporal and intra-loss improve the stylization stability.

**"The proposed framework using 3D CNN for stylization is novel. Results are significantly better compared to baseline methods. The paper expands 200K to 200K.

**"The quality of stylization results is good.**

**"Weaknesses."**

The usage of 3D CNN is not well motivated. - The technical contributions are weak. - Extra compositional cost compared with 2D CNN-based methods. - The model needs to be trained with four cascaded phases. - Look of comprehensive comparison with state-of-the-art methods. - Under necessity of four appearance subsets and Entangle Subnet. - Lack of detailed network structure description - Weak paper writing and confusing logic in Section 3. - Lack of analysis in ablation study. - Linked comparison with related works. - Lack of user study for subjective assessment. - Lack of obvious improvement in quantitative comparison. - Lack of comparison to single image stylization with optical flow motion compensation. - Questions." - Why is 3D CNN used for stylization and what are the motivations for its usage? - Why are four appearance subsets and the Entangle Subnet necessary? - Does does reduce the number of appearance subsets affect performance? - How does the model training in different stages enforce network functionalities? - How to disentangle motion and appearance using four appearance subsets? - "Paper Decision."**

**"Paper Decisions."**

**"Decision: Accept - Reasons: The paper presents a novel method for video stylization with 3D CNNs and 200K, achieving state-of-the-art results. While there are concerns about the motivation for using 3D CNN and the need for more comprehensive comparisons, the reviewers agree that the technical soundness and contributions of the paper warrant acceptance. Further improvements in addressing the reviewers' feedback could enhance the paper's impact.

**"Summary."**

The paper introduces 200, a method that utilizes 3D CNN (K3D) in the encoder backbone for video stylization, with a focus on separating motion and appearance using appearance subsets and 200K. This separation is intended to enhance the stylization stability by ensuring that motion and appearance are not entangled, which is a significant challenge in traditional 2D CNN-based methods. Results are demonstrated on the sportsful dataset, highlighting the superior performance of the proposed method compared to existing baseline methods. Despite the methodological advancements, the paper is criticized for its limited evaluation scope and unclear motivations for using 3D CNNs.

**"Strandhs."**

The paper is the first to apply 3D CNNs to video stylization, which is a significant technical contribution. - The use of a novel framework including an appearance subset and Entangle network demonstrates innovative approaches in handling the stylization process.

The proposed method has been trained in multiple phase, which ensures that the network effectively learns its functionalities.

The results show significant improvements over baseline methods, indicating the effectiveness of the proposed approach.

The paper introduces a large-scale dataset specifically curated for video stylization, enhancing the reproducibility and applicability of the research.

**"Weaknesses."**

The motivation and advantages of using 3D CNNs for video stylization are not well articulated, making the technical contributions seem weaker.

The paper sheds a clear explanation of why four appearance subsets are necessary, and the role of the Entangle Subnet is unclear.

**"The description of the network structure and its functionalities is confusing, particularly in Section 3 of the paper. - The computational cost associated with using 3D CNNs compared to 20 CNNs is not addressed, raising concerns about the scalability and efficiency of the proposed method. - Comparisons with state-of-the-art (SOTA) methods are not comprehensive, focusing only on optical flow metrics which might not adequately capture the quality of style transfer. - There is a lack of user studies, which are critical for evaluating the subjective quality of the stylization results. - Questions." - Can the authors clarify the necessity and role of the multiple appearance subsets and the Entangle Subnet in the proposed method? - How would the performance of the model be affected if the number of appearance subsets were reduced? - Why is a 3D CNN necessary when a 2D CNN could potentially suffice for extracting style features? - Could the authors elaborate on how the proposed method disentangles motion and appearance, particularly in the context of the training phase? - In the context of stylization, what are the advantages of using 3D CNN over a 2D CNN with optical flow motion compensation? - Could the authors provide more detailed comparisons with other SOTA methods, including qualitative and quantitative assessments across a broader range of metrics? - Questions." - Why is 3D CNN used for stylization and what are the motivations for its usage? - Why are four appearance subsets and the Entangle Subnet necessary? - Does does reduce the number of appearance subsets affect performance? - How does the model training in different stages enforce network functionalities? - How to disentangle motion and appearance using four appearance subsets? - Are they the best suited for video stylization in a sense? - Does the method significantly outperforms the existing baseline methods? - Does the method significantly outperforms the existing baseline methods?"Summary."

The paper proposes XXX and XXX, two algorithms for risk-sensitive reinforcement learning (RL) in low-rank Markov Decision Processes (MDP) with nonlinear function approximation. XXX is a representation learning algorithm that optimises the CML metric using an ML model and UCR-type dynamics, while XX is a computationally efficient planning oracle that improves the computational complexity of XXX. Both algorithms are shown to be the first provably efficient algorithms for CVaR in low-rank MDP.

"Stratistics."

" The paper extends the application of risk-sensitive RL to low-rank MDPs with nonlinear function approximation, which is an important step towards applying risk-sensitive RL to real-world problems where the state space is large and function approximation is necessary.

" The proposed XXX algorithm is the first provably simple-efficient form for CVaR. It is now a subtype, and the theoretical analysis of the algorithm demonstrates that it would provide an upper "optimal" optimal CVaR with threshold(1/Hopkdom(2/2) sample).

" The introduction of the XXX algorithm improves the computational complexity of XXX planning, making it only require polynomial running time with an ML oracle.

"Weaknesses."

"The authors provide a comprehensive theoretical analysis of the algorithm, demonstrating its simple efficiency.

" The paper advances the underlying model transitions admit a low-rank decomposition, but it does not provide a method for estimating the rank of the underlying transition kernel.

" The paper assumes the availability of an ML oracle, which may not be practical in all real-world applications.

"The authors propose a novel discretized least-squares Value iteration (SVM) algorithm for the CVaR objective, which is computationally efficient.

"The paper provides theoretical guarantees on the sample complexity and computational complexity of the proposed algorithm.

"The paper is built-written and easy to follow.

"The paper is the first to study CVaR in low-rank MDPs with nonlinear function approximation.

"The paper provides a thorough discussion of the theoretical properties of the proposed algorithm.

"Weaknesses."

The paper does not provide a comparison of its algorithm with existing methods in the field.

"The paper does not provide empirical results to demonstrate the performance of the proposed algorithm.

"The paper does not discuss the limitations of the proposed algorithm.

"The paper does not provide empirical evaluation of the algorithm.

"The paper does not provide empirical evaluation of the algorithm.

"The paper does not provide empirical evaluation of the algorithm.

"Question.""

"Could you provide a comparison of your algorithm with existing methods in the field?"

"Could you provide empirical results to demonstrate the performance of your proposed algorithm?"

"Could you discuss the limitations of your proposed algorithm?"

"Could you provide empirical evaluation of your algorithm?"

Could you provide empirical evaluation of your algorithm?"

Could you provide empirical evaluation of your algorithm?"

Could you provide empirical evaluation of your algorithm?"

Could you provide empirical evaluation of your algorithm?"

Could you provide empirical evaluation of your algorithm?"

Could you provide empirical evaluation of your algorithm?"

"Could you provide empirical evaluation of your algorithm?"

"Soundness."

"Doplices."

"Because."

The paper makes a significant contribution to the field of risk-sensitive RL and provides theoretical guarantees for the sample complexity and computational efficiency of the proposed algorithm.

"The paper focuses on the sample complexity and computational efficiency of the proposed algorithm.

"The paper focuses on the sample complexity and computational efficiency of the proposed algorithm.

"The paper focuses on the sample complexity and computational efficiency of the proposed algorithm.

"Question.""

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

[MISSING_PAGE_POST]

"Question."

"Question."

"Question."

"Question."

"Question."

"Question."

"Question."

"

[MISSING_PAGE_POST]

"
"Question."

"

[MISSING_PAGE_POST]

"
"Question."

"Question."

"Question."

"Question."

"Question."

"Question."

"Question."

"
"Question."

"Question."

"
"Question."

"
"
"Question."

"
"
"
"
"

"
"

"

"

"

"

"

"

"

"

"

"

"

"

"

"

"

"

"

"

"

" "Summary."

The paper investigates the application of Condition Value at Risk (C/AR) in novel markov Decision Processes (MDB), specifically focusing on the integration of exploration bonuses and risk-sensitive policies. The authors propose an algorithm called EIA (ELIA in the appendix), which leverages an MLt oracle to optimize C/AR and achieve near-optimal policies with polynomial sample complexity. The algorithm is further refined by introducing a computationally efficient planning oracle, EIA, which enhances computational efficiency. The theoretical framework provided includes proofs of sample complexity and regret bounds, although there are designed for their clarity and the assumptions made.

"Structure."

"The paper introduces a novel algorithm for C/AR RL in low-rank MDPs, which is the first of its kind to address this specific setting.

"The algorithm is designed to be computationally efficient, with theoretical guarantees for both regret and sample complexity,"

"The algorithm is well-explained and easy to follow, with clear mathematical notation and a comprehensive analysis of the algorithm's theoretical properties.

"The authors provide a comprehensive discussion on the computational complexity of their algorithm, which is a significant contribution to the field.

"The paper addresses a relevant problem in risk-sensitive RL, which is crucial for practical applications in finance and risk management.

"Weaknesses."

"The papers' theoretical analysis, particularly the proofs of regret and sample complexity, is not well-explained, leading to potential misunderstandings about the assumptions and the implications of the results.

"There is a lack of clarity in the definitions and theorems, which could hinder understanding and reproducibility of the results.

"Weaknesses."

"The paper does not sufficiently discuss the limitations of its approach, particularly in terms of computational efficiency and the practical applicability of the assumptions made.

"The paper does not provide empirical evidence or simulations to support the claims made about the algorithm's performance, which could undermine the credibility of the results.

"The papers' reliance on a low-rank assumption and the need for a strong realizability assumption could limit the generalizability of the results.

"Questions."

"Can the authors clarify the assumptions and implications of the regret and sample complexity results, particularly in terms of the assumptions made and the conditions under which the results hold?"

"How does the algorithm handle the computational efficiency issues, especially in large-scale MDPs?"

"i.e. there a possibility to extend the algorithm to handle non-discrete reward distributions, and if so, what are the challenges involved?"

"How does the algorithm compare to existing algorithms in terms of computational efficiency and sample complexity?"

"Could the authors provide more details on the computational efficiency of the algorithm, particularly in scenarios where the state space is larger?"

"What are the practical implications of the assumptions made in the paper, and how do these assumptions affect the generalizability of the results?"

"Soundness."

"S good

"Presentation."

"Goundness."

"Contribution."

"Question."

"Rating."

"S marginally below the acceptance threshold

"Contribution."

"Paper Decision."

"Decision."

"Reasons: The paper, while introducing a novel algorithm for C/AR RL in low-rank MDPs, suffers from several critical issues that led to its rejection. The primary concerns include the lack of clarity in the theoretical analysis, particularly the proofs of regret and sample complexity, which are not well-explained and rely on strong assumptions that may not hold in practical scenarios. Additionally, the paper does not provide empirical evidence to support the claims made about the algorithm's performance, which is essential for validating the theoretical results. Furthermore, the paper's assumptions, such as the realizability assumption and the low-rank assumption, are not adequately discussed, which raises concerns about the generalizability of the results. The decision to reject is supported by the metaverse, which highlights these issues and suggests that the paper needs significant revisions to address these concerns before it can be considered for acceptance.

"Summary."

"The paper investigates risk-sensitive reinforcement learning (RL) in the context of low-rank Markov Decision Processes (MDB), focusing on the Conditional Value at Risk (C/AR) objective. The authors introduce an algorithm called EIA (ELIA in the appendix), which employs an upper Confidence Bound (C/AR) loss-driven approach to manage exploration, exploitation, and representation learning. The algorithm leverages a discretized least-Squares Value Iteration (LS/U) for computational efficiency. The paper provides theoretical analysis on the sample complexity and computational efficiency of the algorithm, showing that it achieves a sample complexity of 0-("- 

**INSTRUCTION:**

As an experienced academic paper reviewer, you are presented with different review contents for the same paper. Please analyze these contents carefully and consolidate them into a single review. The review should be organized into nine sections: Summary, Strengths, Weaknesses, Questions, Soundness, Presentation, Contribution, Rating and Paper Decision. Below is a description of each section:

1. Summary: Combine the 'Summary' sections from all reviews into a cohesive summary, aiming for a length of about 100-150 words.

2. Strengths/Weaknesses/Questions: Combine the Strengths/Weaknesses/Questions sections from all reviews into a unified, cohesive bullet-point list that avoids redundancy while preserving the specific details and depth of each point.

3. Soundness/Presentation/Contribution: Aggregate the Contribution/Soundness/Presentation score from each review to determine a suitable overall score (the score must be an **integer**), then, match this integer score to the corresponding criterion from the list below and provide the result. For example, if the score is 3, the result should be '3 good'. The possible scores and their criteria are:

1 poor \(\verb|n|\) 2 fair \(\verb|n|\) 3 good \(\verb|n|\) 4 excellent

4. Rating: Aggregate the 'Rating' from each review to determine a suitable overall Rating (the Rating must be an **integer**), then, match this integer Rating to the corresponding criterion from the list below and provide the result. For example, if the Rating is 1, the result should be '1 strong reject'. The possible Ratings and their criteria are:

1 strong reject

2 reject, significant issues present

3 reject, not good enough

4 possibly reject, but has redeeming facets

5 marginally below the acceptance threshold

6 marginally above the acceptance threshold

7 accept, but needs minor improvements

8 accept, good paper

9 strong accept, excellent work

10 strong accept, should be highlighted at the conference

5. Paper Decision: It must include the Decision itself (Accept or Reject) and the reasons for this decision which is based on Meta-review, the criteria of originality, methodological soundness, significance of results, and clarity and logic of presentation, etc. Please ensure your Decision (Accept/Reject) matches the value of the 'Decision' key in the JSON, if present.

Here is the template for a review format, you must follow this format to output your review result:

**Summary:**\(\verb|n|\) <Summary content> \(\verb|n|\)

**Strengths:**\(\verb|n|\) <Strengths result> \(\verb|n|\)

**Weaknesses:**\(\verb|n|\) <Weaknesses result> \(\verb|n|\)

**Questions:**\(\verb|n|\) <Questions result> \(\verb|n|\)

**Soundness:**\(\verb|n|\) <Soundness result> \(\verb|n|\)

**Presentation:**\(\verb|n|\) <Presentation result> \(\verb|n|\)

**Contribution:**\(\verb|n|\) <Contribution result> \(\verb|n|\)

**Rating:**\(\verb|n|\) <Rating result> \(\verb|n|\)

**Paper Decision:**

- Decision: Accept/Reject

- Reasons: reasons content

\begin{table}
\begin{tabular}{l l l} \hline \hline
**INSTRUCTION:** & & & \\ \hline As an experienced academic paper reviewer, you are presented with different review contents for the same paper. Please analyze these contents carefully and consolidate them into a single review. The review should be organized into nine sections: Summary, Strengths, Weaknesses, Questions, Soundness, Presentation, Contribution, Rating and Paper Decision. Below is a description of each section:

1. Summary: Combine the ‘Summary’ sections from all reviews into a cohesive summary, aiming for a length of about 100-150 words.

2. Strengths/Weaknesses/Questions: Combine the Strengths/Weaknesses/Questions sections from all reviews into a unified, cohesive bullet-point list that avoids redundancy while preserving the specific details and depth of each point.

3. Soundness/Presentation/Contribution: Aggregate the Contribution/Soundness/Presentation score from each review to determine a suitable overall score (the score must be an **integer**), then, match this integer score to the corresponding criterion from the list below and provide the result. For example, if the score is 3, the result should be ‘3 good’. The possible scores and their criteria are:

1 poor \(\verb|n|\) 2 fair \(\verb|n|\) 3 good \(\verb|n|\) 4 excellent

4. Rating: Aggregate the ‘Rating’ from each review to determine a suitable overall Rating (the Rating must be an **integer**), then, match this integer Rating to the corresponding criterion from the list below and provide the result. For example, if the Rating is 1, the result should be ‘1 strong reject’. The possible Ratings and their criteria are:

1 strong reject

2 reject, significant issues present

3 reject, not good enough

4 possibly reject, but has redeeming facets

5 marginally below the acceptance threshold

6 marginally above the acceptance threshold

7 accept, but needs minor improvements

8 accept, good paper

9 strong accept, excellent work

10 strong accept, should be highlighted at the conference

5. Paper Decision: It must include the Decision itself (Accept or Reject) and the reasons for this decision which is based on Meta-review, the criteria of originality, methodological soundness, significance of results, and clarity and logic of presentation, etc. Please ensure your Decision (Accept/Reject) matches the value of the ‘Decision’ key in the JSON, if present.

Here is the template for a review format, you must follow this format to output your review result:

**Summary:**\(\verb|n|\) <Summary content> \(\verb|n|\)

**Strengths:**\(\verb|n|\) <Strengths result> \(\verb|n|\) **Weaknesses:**\(\verb|n|\) <Weaknesses result> \(\verb|n|\) **Questions:**\(\verb|n|\) <Questions result> \(\verb|n|\)

**Soundness:**\(\verb|n|\) **\(\verb|n|\) <Soundness result> \(\verb|n|\) **Presentation:**\(\verb|n|\) **\(\verb|n|\) <Presentation result> \(\verb|n|\) **Contribution:**\(\verb|n|\) **\(\verb|n|\) **\(\verb|n|\) **Contribution result> \(\verb|n|\) **Rating:**\(\verb|n|\) **\(\verb|n|\) **\(\verb|n|\) **\(\verb|n|\)**Rating result> \(\verb|n|\)**

**Paper Decision:**

- Decision: Accept/Reject

- Reasons: reasons content

**INSTRUCTION:**

You are a highly experienced, conscientious, and fair academic reviewer. Please help me review this paper. The review should be organized into nine sections:

1. Summary: A summary of the paper in 100-150 words.

2. Strengths/Weaknesses/Questions: The Strengths/Weaknesses/Questions of paper, which should be listed in bullet points, with each point supported by specific examples from the article where possible.

3. Soundness/Contribution/Presentation: Rate the paper's Soundness/Contribution/Presentation, and match this score to the corresponding criterion from the list below and provide the result. The possible scores and their criteria are:

1 poor

2 fair

3 good

4 excellent

4. Rating: Give this paper an appropriate rating, match this rating to the corresponding criterion from the list below and provide the result. The possible Ratings and their criteria are:

1 strong reject

2 reject, significant issues present

3 reject, not good enough

4 possibly reject, but has redeeming facets

5 marginally below the acceptance threshold

6 marginally above the acceptance threshold

7 accept, but needs minor improvements

8 accept, good paper

9 strong accept, excellent work

10 strong accept, should be highlighted at the conference

5. Paper Decision: It must include the Decision itself (Accept or Reject) and the reasons for this decision which is based on the criteria of originality, methodological soundness, significance of results, and clarity and logic of presentation.

Here is the template for a review format, you must follow this format to output your review result:

**Summary:**\(\backslash\)n <Summary content> \(\backslash\)n

**Strengths:**\(\backslash\)n <Strengths result> \(\backslash\)n

**Weaknesses:**\(\backslash\)n <Weaknesses result> \(\backslash\)n

**Questions:**\(\backslash\)n <Questions result> \(\backslash\)n

**Soundness:**\(\backslash\)n <Soundness result> \(\backslash\)n

**Presentation:**\(\backslash\)n <Presentation result> \(\backslash\)n

**Contribution:**\(\backslash\)n <Contribution result> \(\backslash\)n

**Rating:**\(\backslash\)n <Rating result> \(\backslash\)n

**Paper Decision:**

- Decision: Accept/Reject

- Reasons: reasons content

Please ensure your feedback is objective and constructive. The paper is as follows: <paper content>

\begin{table}
\begin{tabular}{l} \hline \hline
**INSTRUCTION:** \\ You are a highly experienced, conscientious, and fair academic reviewer. Please help me review this paper. The review should be organized into nine sections:

1. Summary: A summary of the paper in 100-150 words.

2. Strengths/Weaknesses/Questions: The Strengths/Weaknesses/Questions of paper, which should be listed in bullet points, with each point supported by specific examples from the article where possible.

3. Soundness/Contribution/Presentation: Rate the paper’s Soundness/Contribution/Presentation, and match this score to the corresponding criterion from the list below and provide the result. The possible scores and their criteria are:

1 poor
2 fair
3 good
4 excellent
4. Rating: Give this paper an appropriate rating, match this rating to the corresponding criterion from the list below and provide the result. The possible Ratings and their criteria are:

1 strong reject
2 reject, significant issues present

3 reject, not good enough

4 possibly reject, but has redeeming facets

5 marginally below the acceptance threshold

6 marginally above the acceptance threshold

7 accept, but needs minor improvements

8 accept, good paper

9 strong accept, excellent work

10 strong accept, should be highlighted at the conference

5. Paper Decision: It must include the Decision itself (Accept or Reject) and the reasons for this decision which is based on the criteria of originality, methodological soundness, significance of results, and clarity and logic of presentation.

Here is the template for a review format, you must follow this format to output your review result:

**Summary:**\(\backslash\)n <Summary content> \(\backslash\)n

**Strengths:**\(\backslash\)n <Strengths result> \(\backslash\)n

**Weaknesses:**\(\backslash\)n <Weaknesses result> \(\backslash\)n

**Questions:**\(\backslash\)n <Questions result> \(\backslash\)n

**Soundness:**\(\backslash\)n <Soundness result> \(\backslash\)n

**Presentation:**\(\backslash\)n <Presentation result> \(\backslash\)n

**Contribution:**\(\backslash\)n <Contribution result> \(\backslash\)n

**Rating:**\(\backslash\)n <Rating result> \(\backslash\)n

**Paper Decision:**

- Decision: Accept/Reject

- Reasons: reasons content

Please ensure your feedback is objective and constructive. The paper is as follows: <paper content>